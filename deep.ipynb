{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (2, 4, 64)\n",
      "\n",
      "KV latent shape (compressed): (2, 4, 16)\n",
      "\n",
      "Key content shape: (2, 4, 64)\n",
      "Key rotary shape: (2, 4, 32)\n",
      "\n",
      "Final Q shape (content + rotary): (2, 4, 4, 24)\n",
      "Final K shape (content + rotary): (2, 4, 4, 24)\n",
      "V shape: (2, 4, 4, 16)\n",
      "\n",
      "Final output shape: (2, 4, 64)\n",
      "\n",
      "Input shape: (2, 4, 64)\n",
      "\n",
      "KV latent shape (compressed): (2, 4, 16)\n",
      "\n",
      "Key content shape: (2, 4, 64)\n",
      "Key rotary shape: (2, 4, 32)\n",
      "\n",
      "Final Q shape (content + rotary): (2, 4, 4, 24)\n",
      "Final K shape (content + rotary): (2, 4, 4, 24)\n",
      "V shape: (2, 4, 4, 16)\n",
      "\n",
      "Final output shape: (2, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "batch_size = 2\n",
    "sequence_length = 4\n",
    "d_model = 64        # Input/output dimension\n",
    "num_heads = 4       # Number of attention heads\n",
    "d_head = d_model // num_heads  # Dimension per head\n",
    "d_latent = 16      # Compressed latent dimension (much smaller than d_model)\n",
    "d_rotary = 8       # Dimension for rotary component\n",
    "\n",
    "class MultiHeadLatentAttention(keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_latent, d_rotary):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.d_latent = d_latent\n",
    "        self.d_rotary = d_rotary\n",
    "        \n",
    "        # Compression projections for KV\n",
    "        self.kv_down = keras.layers.Dense(d_latent)  # Down projection to latent space\n",
    "        self.k_up = keras.layers.Dense(d_model)      # Up projection for keys\n",
    "        self.v_up = keras.layers.Dense(d_model)      # Up projection for values\n",
    "        \n",
    "        # Rotary component for keys\n",
    "        self.k_rotary = keras.layers.Dense(num_heads * d_rotary)\n",
    "        \n",
    "        # Query projections\n",
    "        self.q_down = keras.layers.Dense(d_latent)   # Down projection for queries\n",
    "        self.q_up = keras.layers.Dense(d_model)      # Up projection for queries\n",
    "        self.q_rotary = keras.layers.Dense(num_heads * d_rotary)  # Rotary for queries\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_linear = keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, rotary=False):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        if rotary:\n",
    "            dim = self.d_rotary\n",
    "        else:\n",
    "            dim = self.d_head\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def apply_rotary_embedding(self, x, seq_length):\n",
    "        # Simplified rotary embedding for demonstration\n",
    "        position = tf.range(seq_length, dtype=tf.float32)\n",
    "        position = tf.expand_dims(position, axis=1)\n",
    "        return x + position * 0.02  # Simple positional modification\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        \n",
    "        print(\"\\nInput shape:\", x.shape)\n",
    "        \n",
    "        # 1. Compress KV into latent space\n",
    "        kv_latent = self.kv_down(x)\n",
    "        print(\"\\nKV latent shape (compressed):\", kv_latent.shape)\n",
    "        \n",
    "        # 2. Generate keys and values from latent\n",
    "        k_content = self.k_up(kv_latent)\n",
    "        v_content = self.v_up(kv_latent)\n",
    "        k_rot = self.k_rotary(x)\n",
    "        print(\"\\nKey content shape:\", k_content.shape)\n",
    "        print(\"Key rotary shape:\", k_rot.shape)\n",
    "        \n",
    "        # 3. Process queries\n",
    "        q_latent = self.q_down(x)\n",
    "        q_content = self.q_up(q_latent)\n",
    "        q_rot = self.q_rotary(x)\n",
    "        \n",
    "        # 4. Split heads for content and rotary components\n",
    "        q_content = self.split_heads(q_content)\n",
    "        k_content = self.split_heads(k_content)\n",
    "        v_content = self.split_heads(v_content)\n",
    "        q_rot = self.split_heads(q_rot, rotary=True)\n",
    "        k_rot = self.split_heads(k_rot, rotary=True)\n",
    "        \n",
    "        # 5. Apply rotary embeddings\n",
    "        q_rot = self.apply_rotary_embedding(q_rot, seq_length)\n",
    "        k_rot = self.apply_rotary_embedding(k_rot, seq_length)\n",
    "        \n",
    "        # 6. Concatenate content and rotary components\n",
    "        q = tf.concat([q_content, q_rot], axis=-1)\n",
    "        k = tf.concat([k_content, k_rot], axis=-1)\n",
    "        \n",
    "        print(\"\\nFinal Q shape (content + rotary):\", q.shape)\n",
    "        print(\"Final K shape (content + rotary):\", k.shape)\n",
    "        print(\"V shape:\", v_content.shape)\n",
    "        \n",
    "        # 7. Compute attention scores\n",
    "        scale = tf.math.sqrt(tf.cast(self.d_head + self.d_rotary, tf.float32))\n",
    "        attention_scores = tf.matmul(q, k, transpose_b=True) / scale\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # 8. Apply attention to values\n",
    "        output = tf.matmul(attention_weights, v_content)\n",
    "        \n",
    "        # 9. Combine heads and final projection\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        output = self.output_linear(output)\n",
    "        \n",
    "        print(\"\\nFinal output shape:\", output.shape)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create and test\n",
    "dummy_input = tf.random.uniform((batch_size, sequence_length, d_model))\n",
    "mla = MultiHeadLatentAttention(d_model, num_heads, d_latent, d_rotary)\n",
    "output, attention_weights = mla(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input (2, 4, 64)\n",
    "   ↓\n",
    "Compression to latent space (2, 4, 16)  # Much smaller!\n",
    "   ↓\n",
    "Up-projection and head splitting\n",
    "   ↓\n",
    "Attention computation\n",
    "   ↓\n",
    "Combine heads\n",
    "   ↓\n",
    "Final output (2, 4, 64)  # Back to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shape: (2, 4, 64)\n",
      "\n",
      "Affinity scores shape: (2, 4, 256)\n",
      "\n",
      "Gating weights shape: (2, 4, 256)\n",
      "Selected experts shape: (2, 4, 8)\n",
      "\n",
      "Example for first sequence in batch:\n",
      "First token's top experts: tf.Tensor([225 255  79 178 159 118 252  76], shape=(8,), dtype=int32)\n",
      "Corresponding gating weights: [0.12765584886074066, 0.12545745074748993, 0.12526875734329224, 0.12515246868133545, 0.12500016391277313, 0.12431211024522781, 0.123655766248703, 0.12349744141101837]\n",
      "\n",
      "Sum of gating weights for first token: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class MoEGating(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts=256, num_selected_experts=8, d_model=64):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_selected_experts = num_selected_experts\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Initialize expert centroids (learnable)\n",
    "        self.expert_centroids = tf.Variable(\n",
    "            initial_value=tf.random.normal([num_experts, d_model], stddev=0.02),\n",
    "            trainable=True,\n",
    "            name=\"expert_centroids\"\n",
    "        )\n",
    "        \n",
    "        # Initialize balance biases\n",
    "        self.balance_biases = tf.Variable(\n",
    "            initial_value=tf.zeros([num_experts]),\n",
    "            trainable=True,\n",
    "            name=\"balance_biases\"\n",
    "        )\n",
    "\n",
    "    def compute_affinity(self, inputs):\n",
    "        # inputs shape: [batch_size, seq_len, d_model]\n",
    "        # expert_centroids shape: [num_experts, d_model]\n",
    "        \n",
    "        # Reshape inputs to [batch_size * seq_len, d_model]\n",
    "        batch_size, seq_len, _ = tf.shape(inputs)\n",
    "        flat_inputs = tf.reshape(inputs, [-1, self.d_model])\n",
    "        \n",
    "        # Compute affinity scores: [batch_size * seq_len, num_experts]\n",
    "        logits = tf.matmul(flat_inputs, self.expert_centroids, transpose_b=True)\n",
    "        \n",
    "        # Add balance biases\n",
    "        logits = logits + self.balance_biases\n",
    "        \n",
    "        # Apply sigmoid\n",
    "        affinities = tf.sigmoid(logits)\n",
    "        \n",
    "        # Reshape back to [batch_size, seq_len, num_experts]\n",
    "        affinities = tf.reshape(affinities, [batch_size, seq_len, self.num_experts])\n",
    "        return affinities\n",
    "\n",
    "    def top_k_gating(self, affinities):\n",
    "        # Get top-k values and indices\n",
    "        top_k_values, top_k_indices = tf.math.top_k(\n",
    "            affinities, k=self.num_selected_experts\n",
    "        )\n",
    "        \n",
    "        # Create a mask for selected experts\n",
    "        mask = tf.one_hot(top_k_indices, depth=self.num_experts)\n",
    "        \n",
    "        # Combine all experts selected for a token\n",
    "        mask = tf.reduce_sum(mask, axis=-2)\n",
    "        \n",
    "        # Mask out non-selected expert affinities\n",
    "        gating = affinities * tf.cast(mask > 0, tf.float32)\n",
    "        \n",
    "        # Normalize gating weights\n",
    "        normalizer = tf.reduce_sum(gating, axis=-1, keepdims=True)\n",
    "        gating = gating / (normalizer + 1e-9)\n",
    "        \n",
    "        return gating, top_k_indices\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute expert affinities\n",
    "        affinities = self.compute_affinity(inputs)\n",
    "        print(\"\\nAffinity scores shape:\", affinities.shape)\n",
    "        \n",
    "        # Compute gating weights and expert selection\n",
    "        gating, selected_experts = self.top_k_gating(affinities)\n",
    "        print(\"\\nGating weights shape:\", gating.shape)\n",
    "        print(\"Selected experts shape:\", selected_experts.shape)\n",
    "        \n",
    "        return gating, selected_experts\n",
    "\n",
    "# Test the implementation\n",
    "\n",
    "num_experts = 256\n",
    "num_selected_experts = 8\n",
    "\n",
    "# Create dummy input (as if coming from MHA)\n",
    "dummy_input = output\n",
    "print(\"\\nInput shape:\", dummy_input.shape)\n",
    "\n",
    "# Create and apply gating\n",
    "gating_layer = MoEGating(num_experts, num_selected_experts, d_model)\n",
    "gating_weights, selected_experts = gating_layer(dummy_input)\n",
    "\n",
    "# Print example output for first sequence in batch\n",
    "print(\"\\nExample for first sequence in batch:\")\n",
    "print(\"First token's top experts:\", selected_experts[0, 0])\n",
    "print(\"Corresponding gating weights:\", \n",
    "      [float(gating_weights[0, 0, idx]) for idx in selected_experts[0, 0]])\n",
    "\n",
    "# Verify sum of gating weights is approximately 1\n",
    "print(\"\\nSum of gating weights for first token:\", \n",
    "      float(tf.reduce_sum(gating_weights[0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final MoE output shape: (2, 4, 64)\n",
      "\n",
      "MoE layer test:\n",
      "Input shape: (2, 4, 64)\n",
      "Output shape: (2, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "class MoELayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts=256, d_model=64, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Shared expert (always used)\n",
    "        self.shared_expert = self._create_ffn()\n",
    "        \n",
    "        # Create routed experts\n",
    "        self.routed_experts = [self._create_ffn() for _ in range(num_experts)]\n",
    "    \n",
    "    def _create_ffn(self):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(self.d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, gating_weights, selected_experts):\n",
    "        batch_size, seq_len, _ = tf.shape(inputs)\n",
    "        \n",
    "        # 1. Apply shared expert to all tokens\n",
    "        shared_output = self.shared_expert(inputs)\n",
    "        \n",
    "        # 2. Initialize expert outputs\n",
    "        expert_outputs = tf.zeros_like(inputs)\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Find where this expert is selected\n",
    "            expert_mask = tf.reduce_any(tf.equal(selected_experts, expert_idx), axis=-1)\n",
    "            expert_mask = tf.cast(expert_mask, tf.float32)\n",
    "            \n",
    "            # Get corresponding gates\n",
    "            expert_gates = gating_weights[..., expert_idx]\n",
    "            \n",
    "            # Process tokens through this expert where it's selected\n",
    "            expert_output = self.routed_experts[expert_idx](inputs)\n",
    "            \n",
    "            # Add weighted output to total\n",
    "            expert_outputs += expert_output * tf.expand_dims(expert_gates * expert_mask, -1)\n",
    "        \n",
    "        # 3. Combine shared and routed outputs with residual connection\n",
    "        final_output = inputs + shared_output + expert_outputs\n",
    "        print(\"\\nFinal MoE output shape:\", final_output.shape)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Create and test MoE layer\n",
    "moe_layer = MoELayer(num_experts=num_experts, d_model=d_model)\n",
    "moe_output = moe_layer(dummy_input, gating_weights, selected_experts)\n",
    "\n",
    "print(\"\\nMoE layer test:\")\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", moe_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After RMSNorm shape: (2, 4, 64)\n",
      "\n",
      "Final output shape: (2, 4, 32000)\n",
      "\n",
      "Sample token probabilities (first token):\n",
      "Sum of probabilities: 1.0\n",
      "Top 5 token probabilities: [3.9051996e-05, 3.9068873e-05, 3.9444534e-05, 4.0344134e-05, 4.0591844e-05]\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='weight'\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Calculate RMS\n",
    "        mean_square = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\n",
    "        x_norm = x * tf.math.rsqrt(mean_square + self.eps)\n",
    "        return self.weight * x_norm\n",
    "\n",
    "class OutputHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Apply final projection to vocabulary size\n",
    "        logits = self.dense(x)\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        return probs\n",
    "\n",
    "# Complete the pipeline with previous components\n",
    "# Assuming we have moe_output from previous step\n",
    "vocab_size = 32000  # Example vocabulary size\n",
    "\n",
    "# 1. Apply RMSNorm\n",
    "rms_norm = RMSNorm()\n",
    "normalized_output = rms_norm(moe_output)\n",
    "print(\"\\nAfter RMSNorm shape:\", normalized_output.shape)\n",
    "\n",
    "# 2. Apply output head (only at the end of all transformer blocks)\n",
    "output_head = OutputHead(vocab_size, d_model)\n",
    "final_output = output_head(normalized_output)\n",
    "print(\"\\nFinal output shape:\", final_output.shape)\n",
    "\n",
    "# Print sample probabilities for first token\n",
    "print(\"\\nSample token probabilities (first token):\")\n",
    "print(\"Sum of probabilities:\", float(tf.reduce_sum(final_output[0, 0])))  # Should be close to 1\n",
    "print(\"Top 5 token probabilities:\", sorted(final_output[0, 0].numpy())[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2068\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2068\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1553 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ deep_seek_model_2               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32000</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,202,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DeepSeekModel</span>)                 │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1553 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ deep_seek_model_2               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32000\u001b[0m)      │    \u001b[38;5;34m21,202,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mDeepSeekModel\u001b[0m)                 │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,202,048</span> (80.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,202,048\u001b[0m (80.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,202,048</span> (80.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,202,048\u001b[0m (80.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MoEGating(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts=256, num_selected_experts=8, d_model=64):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_selected_experts = num_selected_experts\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Initialize expert centroids (learnable)\n",
    "        self.expert_centroids = self.add_weight(\n",
    "            shape=(num_experts, d_model),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            name=\"expert_centroids\"\n",
    "        )\n",
    "        \n",
    "        # Initialize balance biases\n",
    "        self.balance_biases = self.add_weight(\n",
    "            shape=(num_experts,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name=\"balance_biases\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = tf.einsum('bsd,ed->bse', inputs, self.expert_centroids)\n",
    "        logits = logits + self.balance_biases\n",
    "        \n",
    "        # Apply sigmoid\n",
    "        affinities = tf.sigmoid(logits)\n",
    "        \n",
    "        # Get top-k values and indices\n",
    "        top_k_values, selected_experts = tf.math.top_k(\n",
    "            affinities, k=self.num_selected_experts\n",
    "        )\n",
    "        \n",
    "        # Create gating weights\n",
    "        mask = tf.one_hot(selected_experts, depth=self.num_experts)\n",
    "        mask = tf.reduce_sum(mask, axis=-2)\n",
    "        gating = affinities * tf.cast(mask > 0, tf.float32)\n",
    "        gating = gating / (tf.reduce_sum(gating, axis=-1, keepdims=True) + 1e-9)\n",
    "        \n",
    "        return gating, selected_experts\n",
    "\n",
    "class MoELayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts=256, d_model=64, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Shared expert\n",
    "        self.shared_expert = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        # Create routed experts\n",
    "        self.routed_experts = [\n",
    "            tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(d_ff, activation='gelu'),\n",
    "                tf.keras.layers.Dense(d_model)\n",
    "            ]) for _ in range(num_experts)\n",
    "        ]\n",
    "    \n",
    "    def call(self, inputs, gating_weights, selected_experts):\n",
    "        # Apply shared expert\n",
    "        shared_output = self.shared_expert(inputs)\n",
    "        expert_outputs = tf.zeros_like(inputs)\n",
    "        \n",
    "        # Process through experts\n",
    "        for i in range(self.num_experts):\n",
    "            expert_mask = tf.reduce_any(tf.equal(selected_experts, i), axis=-1)\n",
    "            expert_mask = tf.cast(expert_mask, tf.float32)[..., tf.newaxis]\n",
    "            expert_gates = gating_weights[..., i:i+1]\n",
    "            \n",
    "            expert_output = self.routed_experts[i](inputs)\n",
    "            expert_outputs += expert_output * expert_gates * expert_mask\n",
    "        \n",
    "        return inputs + shared_output + expert_outputs\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts=256, num_selected_experts=8, d_model=64, d_ff=256):\n",
    "        super().__init__()\n",
    "        self.gating = MoEGating(num_experts, num_selected_experts, d_model)\n",
    "        self.moe = MoELayer(num_experts, d_model, d_ff)\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=d_model // 4\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Attention\n",
    "        attn_output = self.attention(inputs, inputs, inputs)\n",
    "        x = self.norm(inputs + attn_output)\n",
    "        \n",
    "        # Gating\n",
    "        gating_weights, selected_experts = self.gating(x)\n",
    "        \n",
    "        # MoE\n",
    "        x = self.moe(x, gating_weights, selected_experts)\n",
    "        \n",
    "        # Norm\n",
    "        return self.norm(x)\n",
    "\n",
    "class DeepSeekModel(tf.keras.Model):\n",
    "    def __init__(self, num_blocks=2, vocab_size=32000, d_model=64):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.blocks = [TransformerBlock(d_model=d_model) for _ in range(num_blocks)]\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return self.final_layer(x)\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.Input(shape=(64,))  # sequence length of 4\n",
    "model = DeepSeekModel(num_blocks=2, vocab_size=32000, d_model=64)\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Create the Keras model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
